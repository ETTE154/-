# -*- coding: utf-8 -*-
"""서울시_상권분석.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JWlOYBrq03iuv1U_f8G0kcyVotsf4Pld

### 한글 깨짐 해결
"""



import matplotlib as mpl
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
print(mpl.get_cachedir())
[(f.name, f.fname) for f in fm.fontManager.ttflist if 'Nanum' in f.name]

font_list = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')
[matplotlib.font_manager.FontProperties(fname=font).get_name() for font in font_list if 'Nanum' in font]

import matplotlib.pyplot as plt
import matplotlib.font_manager
plt.rc('font', family='NanumGothicCoding') # 나눔 고딕 폰트 설정

plt.rc('font', family='NanumGothic')
mpl.rcParams['axes.unicode_minus'] = False

"""### 모듈 설정 및 파일 불러오기 """

# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns 
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None) 
pd.set_option('display.max_rows', None)


from matplotlib import font_manager, rc


df2021=pd.read_csv(r"C:/Users/lion1/OneDrive - 동의대학교/바탕 화면/서울시_상권/서울시_우리마을가게_상권분석서비스(신_상권_추정매출)_2021년/서울시_우리마을가게_상권분석서비스(신_상권_추정매출)_2021년.csv",encoding='cp949')

print(df2021.info())

"""### 데이터 분석

### 데이터 값의 개수 확인
"""

plt.figure(figsize=(10,5))
sns.countplot(x=df2021['상권_구분_코드_명'])

plt.show

"""### 점포수 파악 """

service=df2021.groupby('서비스_업종_코드_명').agg({'기준_년_코드':'count', '점포수':sum},as_index=False).reset_index()
service.columns=['서비스_업종','개수','점포수']

service=service.sort_values('점포수',ascending=False).iloc[:20]

plt.figure(figsize=(25,10))
sns.barplot(x=service['서비스_업종'],y=service['점포수'])

"""### 데이터 상관관계 확인 """

all=df2021[['분기당_매출_금액','분기당_매출_건수','주중_매출_금액','주말_매출_금액']].corr()
plt.figure(figsize=(10,10))
# 상관관계를 표시하는 그래프 작석, annot=True => cell값 표시 안함 
# fmt = '.2f' 데이터 타입
sns.heatmap(data = all.corr(), annot=True, 
fmt = '.2f', linewidths=.5, cmap='Blues')

"""### 요일별 매출 추이  """

week=df2021[['분기당_매출_금액','월요일_매출_금액','화요일_매출_금액','수요일_매출_금액','목요일_매출_금액','금요일_매출_금액'
       ,'토요일_매출_금액','일요일_매출_금액']].corr()

plt.figure(figsize=(10,10))
sns.heatmap(data = week, annot=True, 
fmt = '.2f', linewidths=.5, cmap='Blues')

"""### 시간대별 매출 추이   """

time=df2021[['분기당_매출_금액','시간대_00~06_매출_금액','시간대_06~11_매출_금액','시간대_11~14_매출_금액'
            ,'시간대_14~17_매출_금액','시간대_17~21_매출_금액','시간대_21~24_매출_금액']].corr()

plt.figure(figsize=(10,10))
sns.heatmap(data = time, annot=True, 
fmt = '.2f', linewidths=.5, cmap='Blues')

"""### 성별 매출 추이  """

gender=df2021[['분기당_매출_금액','여성_매출_금액','남성_매출_금액']].corr()

plt.figure(figsize=(10,10))
sns.heatmap(data = gender, annot=True, 
fmt = '.2f', linewidths=.5, cmap='Blues')

"""### 연령별 매출 추이 """

age=df2021[['분기당_매출_금액','연령대_10_매출_금액','연령대_20_매출_금액','연령대_30_매출_금액'
           ,'연령대_40_매출_금액','연령대_50_매출_금액','연령대_60_이상_매출_금액']].corr()

plt.figure(figsize=(10,10))
sns.heatmap(data = age, annot=True, 
fmt = '.2f', linewidths=.5, cmap='Blues')

"""## 칼럼 명 변경 한글 ===> 영어  """

# 칼럼중 총매출과 높은 상관관계를 보이는 칼럼. 
X=df2021[[ '분기당_매출_건수', '주중_매출_금액', '주말_매출_금액', '월요일_매출_금액',
       '화요일_매출_금액', '수요일_매출_금액', '목요일_매출_금액', '금요일_매출_금액', '토요일_매출_금액',
       '일요일_매출_금액', '시간대_06~11_매출_금액', '시간대_11~14_매출_금액',
       '시간대_14~17_매출_금액', '시간대_17~21_매출_금액', '남성_매출_금액',
       '여성_매출_금액', '연령대_20_매출_금액', '연령대_30_매출_금액',
       '연령대_40_매출_금액', '연령대_50_매출_금액', '연령대_60_이상_매출_금액', 
       '토요일_매출_건수', '시간대_건수~17_매출_건수',
      '연령대_50_매출_건수']]

y=df2021['분기당_매출_금액']

# 칼럼 이름 변경경
X.columns=[['cnt','weekday_amt','weekend_amt','mon_amt','tue_amt','wed_amt','thu_amt','fri_amt','sat_amt','sun_amt','0611_amt','1114_amt',
           '1417_amt','1721_amt','man_amt','woman_amt','20_amt','30_amt','40_amt','50_amt','60_amt','sat_cnt','1417_cnt','50_cnt']]

"""### 주성분 분석(PCA) """

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()   

X_sca = scaler.fit_transform(X)

from sklearn.decomposition import PCA
pca = PCA(n_components=5) # 주성분을 몇개로 할지 결정
printcipalComponents = pca.fit_transform(X_sca)
pcadf = pd.DataFrame(data=printcipalComponents)
# 주성분으로 이루어진 데이터 프레임 구성

pcadf.head()

sum(pca.explained_variance_ratio_)

from sklearn.linear_model import LinearRegression
from statsmodels.formula.api import ols
from sklearn.model_selection import train_test_split
lr=LinearRegression()

y.columns=[['tamt']] #칼럼 명 영어 변경

X_train, X_test, y_train, y_test = train_test_split(pcadf, y, test_size=0.3, random_state=0)

### degree = 2
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2,
                          include_bias=False)
# poly.fit_transform([[2, 3]])
train_poly=poly.fit_transform(X_train)
test_poly=poly.fit_transform(X_test)

print(train_poly.shape, test_poly.shape)

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_poly, y_train)

# 훈련 데이터를 이용한 정확동
print(lr.score(train_poly, y_train))

# 테스트 데이터를 이용한 정확도
print(lr.score(test_poly, y_test))

# degree=3
poly = PolynomialFeatures(degree=3,
                          include_bias=False)
train_poly=poly.fit_transform(X_train)
test_poly=poly.fit_transform(X_test)

lr = LinearRegression()
lr.fit(train_poly, y_train)

# 훈련 데이터를 이용한 정확동
print(lr.score(train_poly, y_train))

# 테스트 데이터를 이용한 정확도
print(lr.score(test_poly, y_test))

"""- 오버피팅 발생

### 규제적용
"""

# 데이터 전처리(정규화=>train 데이터만 진행)
# degree = 3로 진행
from sklearn.preprocessing import StandardScaler

ss=StandardScaler()
ss.fit(train_poly) # train 데이터만 진행

train_scaled=ss.transform(train_poly)
test_scaled=ss.transform(test_poly)

# 릿지(Ridge) 적용
from sklearn.linear_model import Ridge

rideg=Ridge()
rideg.fit(train_scaled, y_train)

# 훈련 데이터를 이용한 정확동
print(rideg.score(train_scaled, y_train))

# 테스트 데이터를 이용한 정확도
print(rideg.score(test_scaled, y_test))

"""### 릿지(Ridge):L2 """

# alpha을 조정해 결과 확인
train_sorce=[]
test_sorce=[]

alpha_lst=[0.001, 0.01, 1, 100, 1000]

for alpha in alpha_lst:
    rideg=Ridge(alpha=alpha)  # 릿지 모델 생성
    rideg.fit(train_scaled, y_train) # 릿지 모델 훈련
    
    # 훈련 데이터를 이용한 정확도 리스트에 추가
    train_sorce.append(rideg.score(train_scaled, y_train))

    # 테스트 데이터를 이용한 정확도 리스트에 추가
    test_sorce.append(rideg.score(test_scaled, y_test))

plt.plot(alpha_lst, train_sorce, label='train')
plt.plot(alpha_lst, test_sorce, label='test')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend()
plt.show()

"""### 라쏘(Lass) : L1 """

from sklearn.linear_model import Lasso

lasso=Lasso()
lasso.fit(train_scaled, y_train)

# 훈련 데이터를 이용한 정확동
print(lasso.score(train_scaled, y_train))

# 테스트 데이터를 이용한 정확도
print(lasso.score(test_scaled, y_test))

# alpha을 조정해 결과 확인
train_sorce=[]
test_sorce=[]

alpha_lst=[0.001, 0.01, 1, 100, 1000]

for alpha in alpha_lst:
    lasso=Lasso(alpha=alpha, max_iter=1000)  # max_iter: 반복 최대 횟수
    lasso.fit(train_scaled, y_train) # 라소 모델 훈련
    
    # 훈련 데이터를 이용한 정확도 리스트에 추가
    train_sorce.append(lasso.score(train_scaled, y_train))

    # 테스트 데이터를 이용한 정확도 리스트에 추가
    test_sorce.append(lasso.score(test_scaled, y_test))

plt.plot(np.log10(alpha_lst), train_sorce, label='train')
plt.plot(np.log10(alpha_lst), test_sorce, label='test')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend()
plt.show()

"""### 엘라스틱넷 """

from sklearn.linear_model import ElasticNet

ela = ElasticNet(alpha=0.01, l1_ratio = 0.5, max_iter = 100000)
ela.fit(train_scaled, y_train)
print(ela.score(train_scaled, y_train))
print(ela.score(test_scaled, y_test))

print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score

# lr = LogisticRegression()
# lr.fit(X_train, y_train)
# pred= lr.predict(X_test)
# accscr = accuracy_score(y_test, pred)
# print('LogisticRegression accuracy_scroe:%.4f'%(accscr))